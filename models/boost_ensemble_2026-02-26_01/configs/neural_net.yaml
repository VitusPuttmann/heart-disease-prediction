# Configuration for the neural net
hidden_units:
  - 128
  - 32
activation: relu
dropout: 0.15
learning_rate: 0.0001
batch_size: 64
epochs: 50
verbose: 2
seed: 834217
weight_initialization: he_normal
l2_lambda: 0.0001
early_stopping: true
early_stopping_patience: 10
validation_split: 0.2
loss_function: cross_entropy
output_activation: softmax
learning_rate_schedule: cosine_decay
gradient_clipping_norm: 1.0
one_hot: true
augment: false